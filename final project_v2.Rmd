---
title: "final project"
author: "Carlos and Eric"
date: "2024-03-23"
format: 
  html: 
    toc: true
    toc-depth: 2
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Libraries

The libraries we are going to use for the work are the following:

```{r}
library(tidyverse)
library(dplyr)
library(magrittr)
library(scales)
library(RColorBrewer)
library(ggsci)
library(ggthemes)
library(lubridate)
library(viridis)
library(ggrepel)
library(reshape)
library(gridExtra)
library(tidyverse)
library(reshape)
library(viridis)
library(tm)
library(SnowballC)
library(wordcloud)
library(NLP)
library(reshape)
library(widyr)
library(wordcloud2)
library(tidytext)
library(janeaustenr)
library(htmlwidgets)
library(topicmodels)
library(stringr)
```

## Introduction

In this paper we will apply different Text Mining techniques using the different scripts of the Harry Potter films to reveal different patterns and trends in the narrative, the characters and the emotions they may have experienced. Using natural language processing, we will look for insights into plot evolution and emotional development throughout the saga, providing unique insights into one of the most iconic universes of literature and cinema.

Before we continue, we want to warn that this work is made by big fans of the saga, so we will do it with great affection and we apologise in advance if we are too much of a fan. It is also necessary to warn that we may make some spoilers but we promise that they will be small (not related to the plot). For that reason, we recommend that those who read us, watch the movies before, or better yet, read the books. You will thank us when you finish them.

## Databases

La base de datos que utilizaremos en este proyecto se ha recopilado de GitHub y está accesible directamente a través del siguiente enlace: [GitHub - HP Dataset](https://github.com/Kornflex28/hp-dataset/tree/main/datasets).

```{r}

hp1 <- read_csv("hp1.csv")
hp2 <- read_csv("hp2.csv")
hp3 <- read_csv("hp3.csv")
hp4 <- read_csv("hp4.csv")
hp5 <- read_csv("hp5.csv")
hp6 <- read_csv("hp6.csv")
hp7 <- read_csv("hp7.csv")
hp8 <- read_csv("hp8.csv")

# Fix misspelling of movie#4
hp4 <- hp4 |> 
  mutate(movie = str_replace_all(string = movie, pattern = "Gobelt", replacement = "Goblet"))

df <- rbind(hp1,hp2,hp3,hp4,hp5,hp6,hp7,hp8)
```

## Initial Hypothesis

Some of the questions we are going to address in the paper are:

-   Who are the characters that have made the greatest impact on popular culture?
-   Is the number of words related to the length of the films?
-   What are the most distinctive words or characters in each film?

## 1. TF-IDF

### Most sentences in movies

One of the most useful tools in Text Mining is the word count in each text to determine how relevant a certain word or topic may be within a corpus. This approach allows us to identify key terms, frequencies and patterns that emerge in the discourse, offering a solution to discover the predominant themes and relative importance of different concepts throughout the narrative.

However, the count is also useful to identify who are the main characters in different novels or in this case, films. Whereas those people who show the highest number of scripted lines in a film should be the main characters.

That's going to be the first step in our work, identifying the main characters of the different films. Luckily, we are big fans of the saga and we will be able to check the results quite easily, but we could do it with any script to know its importance.

```{r}
# Save df as a dataframe with variables 'character' and 'movie'
Char_Dial <- data.frame(table(df$character, df$movie))

# Sum lines for each character throughout all movies
Char_Dial_Sum <- Char_Dial %>%
  group_by(Var1) %>%
  summarise(Total_Freq = sum(Freq)) %>%
  ungroup()

# Select top 10 characters with the most spoken lines
Char_Dial_Top10 <- Char_Dial_Sum %>%
  arrange(desc(Total_Freq)) %>%
  slice_max(Total_Freq, n = 10)

# Create a graph for the top 10 characters with the most lines
ggplot(Char_Dial_Top10, aes(x = reorder(Var1, Total_Freq), y = Total_Freq)) +
  geom_bar(stat = "identity", width = 0.62, fill = "steelblue") +
  coord_flip() +
  labs(title = "Characters with the most sentences",
       subtitle = "Top 10 across all parts of a movie series",
       x = "Character", y = "Number of sentences") +
  theme_minimal() +
  theme(legend.position = "none")  # Remove legend because it is not relevant


```

En este gráfico podemos observar quienes son los 10 personajes más protagonistas en la saga de Harry Potter.

As you might expect, the character who has the most lines in the films is the one who appears in the title of the films and is known as 'The Chosen One' or Harry Potter, to those who aren't such big fans. Harry is followed by his best friends, `Ron Weasly` and `Hermione Granger`, completing the `Golden Trio`.

However, when reviewing the results and as fans of the saga we are struck by the appearance of a particular character `Horace Slughorn` this character appears in the sixth installment having great prominence only in this one, being more forgotten in the last two. In addition, in this list there are great forgotten characters such as `Draco Malfoy` who despite being a very important character in the saga, being one of the main enemies of Harry Potter, is not in the top 10. This could be an indication of the great impact that this character had on popular culture, as everyone who has seen the films or read the books remembers this character, but instead he hardly appears on screen, according to the results obtained.

Next, we are going to divide this analysis by films, to observe how the phrases are distributed throughout the saga. To do this, we will store the names of the characters mentioned in the following vector, with the licence to change `Horace Slughorn` to `Draco Malfoy`.

```{r}
top_characters <- c("Harry Potter", "Ron Weasley", "Hermione Granger", "Albus Dumbledore",  "Rubeus Hagrid", "Severus Snape", "Minerva McGonagall", "Voldemort","Neville Longbottom", "Draco Malfoy")



Char_Dial <- data.frame(table(df$character, df$movie))
Char_Dial %>%
  arrange(desc(Freq)) %>%
  filter(Var1 %in% top_characters) %>%
  ggplot(aes(reorder(Var1, +Freq), Freq, fill = Var2)) +
  geom_bar(stat = "identity", width = 0.62)+
  scale_fill_uchicago()+
  coord_flip()+
  guides(fill = guide_legend(title.position = "top", reverse = T))+
  labs(title = "Characters with the most sentences",
       subtitle = "Top 10, by movie", fill = "Movie",
       x = "Character", y = "Number of sentences")+
  theme_minimal()+
  theme(legend.title.align = 0.5, legend.position = "right", legend.direction = "vertical") 

```

### Most used Spells

In this section, we are going to talk about magic, more specifically spells. In the world of Harry Potter, in order to do magic, you have to cast a spell in a certain way. That is why we are going to see which are the most used spells in the saga.

To do this, first of all, we are going to store in a vector all the spells that are mentioned in the films. To see where we have taken the spells from, click on [here](https://screenrant.com/harry-potter-spells-list-from-movies-and-books/).

```{r}
spells <- c('Accio', 'Alohomora', 'Avada Kedavra', 'Crucio', 'Expecto Patronum', 'Expelliarmus', 'Imperio',
 'Lumos', 'Obliviate', 'Petrificus Totalus', 'Reparo', 'Riddikulus', 'Sectumsempra', 'Wingardium Leviosa')

# We add a column to identify the spell mentioned in each dialogue.
df$spell <- NA  # Initialize the variable with `NA`

for(spell in spells) {
  df$spell <- ifelse(grepl(spell, df$dialog, ignore.case = TRUE), spell, df$spell)
}

# We calculate the count of each spell.
spell_counts <- df %>%
  filter(!is.na(spell)) %>% # Exclude lines without spells
  count(spell, sort = TRUE) # Count occurances for each spell and sort in order

library(viridis) # Make sure to have this package installed


ggplot(spell_counts, aes(x = reorder(spell, n), y = n, fill = spell)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n), position = position_dodge(width = 0.9), hjust = -0.1, size = 3.5) + 
  coord_flip() +
  scale_fill_viridis(discrete = TRUE, option = "D") + 
  labs(title = 'Spells most commonly used',
       subtitle = "Frequency of mentioning spells in dialogues",
       x = 'Spells',
       y = 'Frequency') +
  theme_minimal() +
  theme(legend.title = element_blank(), 
        axis.title.x = element_text(size = 12, face = "bold"),
        axis.title.y = element_text(size = 12, face = "bold"),
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 10),
        legend.position = "none") 
```

The most used spells are `Expeliarmus` and `Expecto Patronum` with a total of 12 times throughout all 8 movies. But let's see how they are distributed across the series.

```{r}
Spels_df <- data.frame(table(df$character, df$movie, df$spell))

Spels_df %>%
  arrange(desc(Freq)) %>%
  filter(Var3 %in% spells) %>%
  ggplot(aes(reorder(Var3, +Freq), Freq, fill = Var2)) +
  geom_bar(stat = "identity", width = 0.62) +
  scale_fill_brewer(palette = "Set2") + 
  coord_flip() +
  guides(fill = guide_legend(title.position = "top", title = "Movie Part")) +
  labs(title = "Spells most commonly used",
       subtitle = "Frequency of mentioning spells by movie",
       x = "Spells",
       y = "Number of appareances") +
  theme_minimal() +
  theme(legend.title.align = 0.5, 
        legend.position = "right", 
        legend.direction = "vertical",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        legend.text = element_text(size = 10)) 

```

Next, we are going to do the same thing but this time instead of separating by films by the number of films they appear in:

```{r}
Spels_df |> 
  filter(Freq > 0) |> 
  select(Var2, Var3) |> 
  distinct() |> 
  count( Var3) |> 
  ggplot(aes(reorder(Var3, +n), n, fill = n)) +
  geom_bar(stat = "identity", width = 0.62) +
  #scale_fill_manual(values = c("peru", "gray90")) + 
  coord_flip() +
  guides(fill = guide_legend(title.position = "top", title = "Movie Part")) +
  labs(title = "Riddikulus is in only one movie",
       subtitle = "Number of movie mentioning spells",
       x = "Spells",
       y = "Number of appareances") +
  theme_minimal() +
  theme(legend.title.align = 0.5, 
        legend.position = "right", 
        legend.direction = "vertical",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        legend.text = element_text(size = 10)) 
```

Having seen how the spells are distributed by film, another way to visualise it is to divide it up according to who the characters are who conjure them. Let's get down to it:

```{r,  fig.width=15, fig.asp=0.65, out.width='100%', preview = TRUE}

spell_character_counts <- df %>%
  filter(spell %in% spells) %>% 
  count(spell, character) %>%
  arrange(spell, desc(n))

# Creando el gráfico
ggplot(spell_character_counts, aes(x = reorder(character, n), y = n, fill = character)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ spell, scales = "free_y") + 
  coord_flip() + 
  scale_fill_viridis_d(begin = 0.2, end = 0.8, direction = -1, option = "C") + 
  labs(title = "Character Spell Usage",
       x = "Character",
       y = "Frequency of Spell Usage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        strip.text.x = element_text(face = "bold"),
        legend.position = "none") 
```

Here we can see the distribution of the different spells cast by the different characters. We can see how most of them are dominated by either `Harry Potter` or `Hermione Granger`. Or in the case of the 'Unforgivable Curses' (for non-fans, we are referring to spells that are forbidden in the Harry Potter world, such as `Avada Kedabra`), predominate `Voldemort` and other dark wizards from the saga. This serves as a great indicator of the relevance of these characters, as well as showing who other influential characters are in the plot.

At this point, we have already found out which spells are used the most and which characters speak the most. The next step is to observe which words are repeated the most and calculate their frequency.

### Most used Words

The first step is to check which film is the most scripted or the longest. That is to say, we will assimilate that the films that have the most dialogue are those that offer us the most minutes on the big screen.

```{r}
library(dplyr)

total_dialogs <- df %>%
  group_by(movie) %>%
  summarize(total_dialogs = n()) # Count the number of rows per group, which is equivalent to counting dialogues.

total_dialogs

```

Let's represent it in a graph.

```{r}

ggplot(total_dialogs, aes(x = reorder(movie, -total_dialogs), y = total_dialogs, fill = movie)) +
  geom_bar(stat = "identity") +
  coord_flip() + # Barras horizontales
  labs(title = "Total Dialogs by Harry Potter Movie",
       x = "Movie",
       y = "Total Dialogs") +
  theme_minimal() +
  theme(legend.position = "none") 

```

Looking at the results and comparing it with the length of the films, we can see that it does not match, i.e. the film with the most dialogue is `Harry Potter and the Order of the Phoenix`, being the second shortest film of the saga. The information about the duration of the films is the following, although you can find more information about it, in the following [link](https://www.pottertalk.net/harry-potter-movie-lengths/).

-   Sorcerer’s Stone = 152 minutes = 2 hours 32 minutes

-   Chamber of Secrets = 161 minutes = 2 hours 41 minutes

-   Prisoner of Azkaban = 142 minutes = 2 hours 22 minutes

-   Goblet of Fire = 157 minutes = 2 hours 37 minutes

-   Order of the Phoenix = 139 minutes = 2 hours 18 minutes

-   Half Blood Prince = 153 minutes = 2 hours 33 minutes

-   Deathly Hallows pt 1 = 146 minutes = 2 hours 26 minutes

-   Deathly Hallows pt 2 = 130 minutes = 2 hours 10 minutes

There doesn't seem to be that much of a relationship between the amount of dialogue and the number of minutes in the film. Let's do the same, but instead of dialogue for words.

```{r}

words <- df %>%
  unnest_tokens(word, dialog) %>%
  count(movie, word, sort = TRUE)

movie_words <-  df %>%
  # we tokenize as usual (as an exception we won't be filtering stopwords now)
  unnest_tokens(word, dialog) %>%
  count(movie, word, sort = TRUE) |> 
   group_by(movie) %>% 
  summarize(total_words = sum(n))
movie_words

ggplot(movie_words, aes(x = reorder(movie, -total_words), y = total_words, fill = movie)) +
  geom_bar(stat = "identity") +
  coord_flip() + # Barras horizontales
  labs(title = "Total Words by Harry Potter Movie",
       x = "Movie",
       y = "Total Words") +
  theme_minimal() +
  theme(legend.position = "none")

```

Again, we see that although the number of words is more closely related to the length of the film, it does not coincide, so we can discard the hypothesis that the longer the dialogue, the longer the film.

Once we have checked this, the next step is to look at the term frequency of each word, to see which words are the most representative of each film. This is calculated as the number of times the word is repeated in the film divided by the total number of words in the film.

```{r}
movie_words <- words %>%
 left_join(movie_words, by = "movie") |> 
  #we add a column for term_frequency in each novel
  mutate(term_frequency = n/total_words)
movie_words

ggplot(movie_words, aes(x = term_frequency)) +
  geom_histogram(binwidth = 0.0001, fill = "#0073C2FF", color = "black") +
  xlim(NA, 0.009) + # Límites en el eje x para enfocar hasta 0.01
  scale_y_continuous(breaks = seq(0, 7000, by = 500)) + # Ajusta los breaks del eje y
  labs(title = "Distribution of Term Frequency Across All Movies",
       x = "Term Frequency (as a percentage of total words)",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

This graph is a histogram showing the distribution of the frequency of terms in dialogue across all the Harry Potter films. Each bar represents the number of terms (y-axis) that occur with a certain frequency (x-axis) within the total set of words in the films. It can be seen that there are a large number of words with low frequency, suggesting that there are a large number of words that are not repeated.

Now we are going to observe a frequency distribution but per film, looking at which film is richer in vocabulary.

```{r}
ggplot(movie_words, aes(x = term_frequency, fill = movie)) +
  geom_histogram(bins = 30, position = "identity") + # Eliminamos la transparencia con alpha
  scale_x_continuous(limits = c(NA, 0.0009), labels = scales::percent_format(accuracy = 0.01)) +
  scale_fill_manual(values = c("Harry Potter and the Chamber of Secrets" = "#1f77b4",
                                "Harry Potter and the Deathly Hallows Part 1" = "#ff7f0e",
                                "Harry Potter and the Deathly Hallows Part 2" = "#2ca02c",
                                "Harry Potter and the Goblet of Fire" = "#d62728",
                                "Harry Potter and the Half-Blood Prince" = "#9467bd",
                                "Harry Potter and the Order of the Phoenix" = "#8c564b",
                                "Harry Potter and the Philosopher's Stone" = "#e377c2",
                                "Harry Potter and the Prisoner of Azkaban" = "#7f7f7f")) + # Añade más colores según sea necesario
  labs(title = "Distribution of Term Frequency Across Movies",
       x = "Term Frequency (as a percentage of total words)",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "right", legend.title = element_blank())
```

We can observe that the film richest in vocabulary is `Harry Potter and the Half-Blood Prince`. However, the distribution of the different films is not clear.

```{r,  fig.width=15, fig.asp=0.65, out.width='100%', preview = TRUE}
# Vizualization
ggplot(movie_words, aes(x = term_frequency, fill = movie)) +
  geom_histogram(binwidth = 0.0001, color = "white") + 
  scale_x_continuous(limits = c(NA, 0.003), labels = scales::percent_format(accuracy = 0.0001)) + 
  facet_wrap(~ movie, ncol = 2, scales = "free_y") + 
  scale_fill_brewer(palette = "Set3") + 
  labs(title = "Term Frequency Distribution by Movie",
       x = "Term Frequency",
       y = "Count") +
  theme_light() + # Aplicamos un tema claro
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1), 
        strip.background = element_rect(fill = "lightblue"), 
        strip.text.x = element_text(size = 8, color = "navy"))
```

In general, we can observe that all the films present a similar distribution.

### TF-IDF

The next step in our analysis is to use the term frequency-inverse document frequency (tf-idf) technique to highlight words that are distinctive in each film in the Harry Potter series. Tf-idf is useful because it helps us to identify not only the most frequent words, but also those that are particularly significant in a given document in relation to a collection of documents. This allows us to look beyond mere frequency and consider the relevance of a term, giving us a more nuanced view of how language is used in the different films.

By calculating the tf-idf of each term in the context of each film, we can filter and visualise the 20 most characteristic words per film, giving us a list of distinctive terms that define or are emblematic of each film.

```{r,  fig.width=15, fig.asp=0.65, out.width='100%', preview = TRUE}
#we create a new variable with the analysis
movie_tf_idf <- movie_words %>%
  bind_tf_idf(word, movie, n) |> 
  select(-total_words) %>%
  #we arrange by tf-idf in descending order
  arrange(desc(tf_idf))

# Vizualization
movie_tf_idf %>%
  group_by(movie) %>%
  slice_max(tf_idf, n = 20) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = movie)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ movie, ncol = 2, scales = "free") +
  scale_fill_brewer(palette = "Set2") + 
  labs(x = "TF-IDF", y = "Words") + 
  theme_minimal() + 
  theme(axis.text.y = element_text(angle = 0), 
        strip.background = element_rect(fill = "lightblue"), 
        strip.text.x = element_text(size = 10, color = "navy"))
```

This analysis is quite representative and is of great help in identifying very relevant factors in the plot. Moreover, when we look at the results, they are quite faithful to reality, because we as fans can state that in most cases, the words with the highest tf-idf are very important in the film.

For example, in `Harry Potter and the Deathly Hallows Part 1` the most distinctive word is Dobby, this character, without going into spoilers, has one of the most important and moving scenes of the whole saga in this film. On the other hand, if we look at the fifth instalment of the saga ( `Harry Potter and the Order of the Phoenix`), the most distinctive word is `prohesy` which makes sense because the whole film revolves around a very important prophecy in the story. Likewise, in the third instalment `Harry Potter and the Prisoner of Azkaban` the most important word is `Pettigrew` and `dementos`, both of which are quite relevant to this film. If the person reading this is a fan, he or she will agree with the results.

Finally, at the beginning of the analysis we were surprised by the sequences of dialogue that `Horace Slughorn` had, as he only appears in 3 of the 8 films, but apparently this character is very distinctive in the sixth film, with his name and surname being the most important words in the whole film.

########## 

# Sentiment analysis

```{r movies-order}
movie_order <- tribble(~num, ~movie, ~film.name,
  1 , "Harry Potter and the Philosopher's Stone", "1-Philosopher's Stone",
  2, "Harry Potter and the Chamber of Secrets", "2-Chamber of Secrets",
  3, "Harry Potter and the Prisoner of Azkaban", "3-Prisoner of Azkaban",
  4, "Harry Potter and the Goblet of Fire", "4-Goblet of Fire",
  5, "Harry Potter and the Order of the Phoenix", "5-Order of the Phoenix",
  6, "Harry Potter and the Half-Blood Prince", "6-Half-Blood Prince",
  7, "Harry Potter and the Deathly Hallows Part 1", "7-Deathly Hallows Part 1",
  8, "Harry Potter and the Deathly Hallows Part 2", "8-Deathly Hallows Part 2"
  )

df <- df |> 
  left_join(movie_order, by = "movie")
```

```{r setup-sentiment-data}

# Bing <- get_sentiments("bing")
# Bing$sentiment <- first(Bing$sentiment)

bing_sentiment <- df %>%
  unnest_tokens(output = word, input = dialog) %>%
  inner_join(get_sentiments("bing"), "word")

affinn_sentiments <- df |>
  mutate(linenumber = row_number(), .by = film.name)  %>% 
  unnest_tokens(output = word, input = dialog) %>%
  inner_join(get_sentiments("afinn")) %>% 
  mutate(method = "AFINN")
```

### Sentiment by chunks of dialogue

```{r plot-affinn-100}
affinn_sentiments |> 
  group_by(film.name,  index = linenumber %/% 100) %>% # split movie dialogue into chunks of lines. 
  summarise(sentiment = sum(value)) |> # sum up the sentiment 
  ggplot(aes(index, sentiment, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  scale_fill_gradient2(high = "darkgreen", 
                       midpoint = 0,
                       low = "red3")+
  facet_wrap(~film.name , ncol = 2, scales = "free_x") +
  theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_blank(), 
    strip.text = element_text(face = "bold", hjust = 0),
    panel.border = element_rect(fill = NA, color = "gray20")
  ) +
  labs(
    x = "Each chunk split up by 100 lines",
    subtitle = "AFFINN method"
  )
```

When splitting up the dialogue into 100-line chunks, you can see which movies have the most positive and negative sentiment. The 6th movie appears to be the most positive. One chunk in the early middle part of the movie has the highest rating of all. The most negative chunk of dialogue is at the middle of the 2nd movie. The 7th movie appears to be neutral throughout and the 8th movie is mostly negative. The first two movies end very positively, unlike any of the others.

Now let's get more detailed.

```{r plot-affinn-8}
affinn_sentiments |> 
  group_by(film.name,  index = linenumber %/% 20) %>% 
  summarise(sentiment = sum(value)) |> 
  ggplot(aes(index, sentiment, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  scale_fill_gradient2(high = "darkgreen", midpoint = 0,
                       low = "red3") +
  facet_wrap(~film.name , ncol = 2, scales = "free_x") +
  theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_blank(), 
    strip.text = element_text(face = "bold", hjust = 0),
    panel.border = element_rect(fill = NA, color = "gray20")
  ) +
  labs(
    x = "Each chunk split up by 20 lines",
    subtitle = "AFFINN method"
  ) 
```

Now we can see more detailed changes in sentiment with 20-line chunks. The 6th movie is still positive, especially in the middle, but now we see a few negative dialogue between the positive ones. It is not *all* positive. We see that the 7th movie has both positive and negative dialogues but they cancel each other out when grouping into large chunks. The most positive sentiment happens at the end of the 1st movie. The 3rd movie starts very positive, as we can see from the 100-line chunks, but now we see that there are some really negative dialogues in the second half of the movie. The most negative dialogues appear to be in the 3rd film.

Now, let's see about grouping it by chapter:

```{r plot-affinn-chapter}
affinn_sentiments |> 
  group_by(film.name, chapter) %>% # group by chapter 
  summarise(sentiment = sum(value), index = min(linenumber)) |>
  arrange(index) |> 
  ggplot(aes(reorder(chapter, index), 
             sentiment, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  scale_fill_gradient2(high = "darkgreen", midpoint = 0,
                       low = "red3")+
  facet_wrap(~film.name , ncol = 2, scales = "free_x") +
  theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_blank(), 
    strip.text = element_text(face = "bold", hjust = 0),
    panel.border = element_rect(fill = NA, color = "gray20")
  )+
  labs(
    x = "Each chunk split up by chapter",
    subtitle  = "AFFINN method"
  )
```

The most positive chapter is in the 6th movie, but close is in the 3rd. Most nagative chapter is in the 2nd. Now we can see one very positive chapter towards the end of teh 8th movie. We also see that the negative detailed chunks in the middle of the 6th movie is gone. All the chapters in the middle

The three methods of looking at sentiment analysis shows that we will get different results from different methods of grouping text. The first method, large chunks of dialogue, gives an overall feeling for long portions of the movie, but hides the mood of a specific scene. The second method, small chunks of dialogue, shows you the detailed view of sentiment which is cancelled out in the first method. The last method, grouping dialogue by chapter, provides a more natural analysis of the flow of each movie. The writers and director meant for each chapter to have a certain feeling, which can be lost when cutting a movie into chunks of dialogue arbitrarily.

### Bigrams

```{r build-bigram}
df_bigrams <- df %>%
  #we take the dialogue in df, and tokenize it to sequences of 2 words
  unnest_tokens(bigram, dialog, token = "ngrams", n = 2) %>%
  drop_na(bigram)

df_bigrams %>%
  count(bigram, sort = TRUE)

bigrams_separated <- df_bigrams |> 
  # separate each bigram in two columns, word1 and word2
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

```

After removing all stopwords from the list of bigrams, we see a lot of names: Harry Potter, Professor Dumblefore, Sirius Black, and so on. The second most used bigram is "ha ha," which I can only assume is referring to someone laughing. Also in the top ten is "bloody hell," which is an exclamation and "dark arts."

```{r bigram}
bigrams_united %>% 
  count(bigram, sort = T) %>% 
  slice_max(n, n=15) %>%
  ggplot(aes(y = reorder(bigram, n), x = n))+
  geom_bar(stat = "identity", width = 0.65, fill = "peru", alpha = 0.85)+
  labs(title = "Most popular bigrams in the movies",
       subtitle = "Top 15",
       y = NULL, x = "Frequency")+
  theme_minimal()
```

```{r}
negation_words <- c("not", "no", "never", "without", "neither", "nor")

not_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

```

```{r not-word-plot}
not_words %>%
  mutate(contribution = n * value,
         sign = if_else(value > 0, "postive", "negative")) %>%
  group_by(word1) %>% 
  slice_max(abs(contribution), n=10) %>%
  ungroup() %>%
  ggplot(aes(y = reorder_within(word2, contribution, word1), 
             x = contribution, 
             fill = contribution)) +
  geom_col() + 
  geom_vline(xintercept = 0, alpha = .3) +
  scale_y_reordered() + 
  facet_wrap(~ word1, scales = "free_y", ncol = 4) + 
  labs(y = 'Words preceeded by a negation',
       x = "Contribution (Sentiment value * number of mentions)",
       title = "Most common pos or neg words to follow negations") +
  scale_fill_gradient2(high = "darkgreen", 
                       midpoint = 0,
                       low = "red3")+
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold", hjust = 0),
    panel.border = element_rect(fill = NA, color = "gray20"),
    legend.position = "none"
  ) 
```

### Trigrams

```{r building-trigrams}

df_trigrams <- df %>%
  #we take the dialogue in df, and tokenize it to sequences of 2 words
  unnest_tokens(trigram, dialog, token = "ngrams", n = 3) %>%
  drop_na(trigram)

df_trigrams %>%
  count(trigram, sort = TRUE)

trigrams_separated <- df_trigrams |> 
  # separate each trigram in three columns, word1 and word2 and word3
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

trigrams_filtered %>% 
  count(word1, word2, word3, sort = TRUE)

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")
```

```{r trigram}
trigrams_united %>% 
  count(trigram, sort = T) %>% 
  slice_max(n, n=10) %>%
  ggplot(aes(y = reorder(trigram, n), x = n))+
  geom_bar(stat = "identity", width = 0.65, fill = "peru", alpha = 0.85)+
  labs(title = "Most popular trigrams in the movies",
       subtitle = "Top 15",
       y = NULL, x = "Frequency")+
  theme_minimal()
```

```{r}
bing_sentiment %>% 
  group_by(word, sentiment) %>%
  summarise(count = n()) |> 
  ungroup() %>%
  arrange(desc(count)) %>%
  slice(1:20) %>%
  ggplot(., aes(y = reorder(word, count), x = count, fill = sentiment))+
  geom_bar(stat = "identity", width = 0.62)+
  scale_fill_brewer(palette = "Set1", type = "div") +
  labs(title = "Most popular words with assigned sentiment",
       subtitle =  expression("Top 20 words"),
       x = "Word", y = "Frequency", fill = "Sentiment")+
  guides(fill = guide_legend(reverse = T))+
  theme_minimal()+ 
  theme(legend.position = "top")
```

```{r}
bing_sentiment %>% 
  group_by(movie, sentiment) %>%
  summarise(count = n())%>%
  ungroup() |> 
   ggplot(aes(y = movie, x = count, fill = sentiment)) +
  geom_bar(stat = "identity", position = "fill", width = 0.7, alpha = 0.9)+
  scale_fill_brewer(palette = "Set1")+
 #  scale_y_continuous(labels = scales::percent)+
  labs(title = "Share of words with positive and negative sentiment",
       subtitle = "by movie", fill = "Sentiment",
       x = "Percentage", y = "Ratio")+
  guides(fill = guide_legend(reverse = T))+
  theme_minimal()+
  theme(legend.position = "top")
```

```{r}
bing_sentiment %>% 
  filter(character %in% c("Harry Potter", "Ron Weasley", "Hermione Granger", "Rubeus Hagrid", "Albus Dumbledore", "Remus Lupin", "Minerva McGonagall", "Draco Malfoy", "Severus Snape", "Lucius Malfoy", "Voldemort", "Tom Riddle", "Sirius Black", "Neville Longbottom"))  %>%
  group_by(character, sentiment) %>%
  summarise(count = n(), .groups = 'drop') %>%
ggplot(., aes(character, count, fill = sentiment))+
  geom_bar(stat = "identity", position = "fill", width = 0.57, alpha = 0.9)+
  scale_x_discrete(limits = c("Harry Potter", "Ron Weasley", "Hermione Granger", "Rubeus Hagrid", "Albus Dumbledore", "Remus Lupin", "Minerva McGonagall", "Draco Malfoy", "Severus Snape", "Lucius Malfoy", "Voldemort", "Tom Riddle", "Sirius Black", "Neville Longbottom"))+
  scale_fill_brewer(palette = "Set1")+
  scale_y_continuous(labels = scales::percent)+
  coord_flip()+
  geom_hline(yintercept = 0.5)+ 
  labs(title = "Share of words with positive and negative sentiment",
       subtitle = "by character (top 15 characters with the most sentences)", fill = "Sentiment",
       x = "Character", y = "Ratio")+
  guides(fill = guide_legend(reverse = T))+
  theme_minimal()+
  theme(legend.title.align = 0.5, legend.position = "right", legend.direction = "vertical")
```

## TOPIC MODELLING

En una saga con múltiples películas, como Harry Potter, donde los personajes, los argumentos y los motivos evolucionan a lo largo del tiempo, el modelado de temas podría identificar patrones y cambios temáticos en la narrativa. Esto podría revelar cómo ciertos temas son introducidos, cómo se desarrollan a lo largo de las películas y qué películas se centran más en ciertos aspectos de la historia o los personajes. La ventaja es que, en lugar de mirar palabras individuales como en el análisis TF-IDF, estaríamos examinando patrones de palabras que representan temas, lo que podría proporcionar una comprensión más profunda del contenido y la estructura de las narrativas de la serie.

En primer lugar vamos a preparar la dataset con la que vamos a trabajar a lo largo del apartado. Para ello, utilizaremos los capítulos que hacen referencia a the chapter of the movie according to the script

```{r}
library(stringr)

# divide into documents, each representing one chapter
by_chapter <- df %>%
  unite(document, movie, chapter)

# tokenize
by_chapter_word <- by_chapter %>%
  unnest_tokens(word, dialog)

# find document-word counts
word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE)

word_counts
```

We have now a tidy dataframe with document, word, and how many times the word appears in the document. El siguiente paso es aplicar el LDA a los capítulos.

### LDA

En primer lugar, vamos a observar el grado de sparsity.

```{r}
chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

chapters_dtm
```

Los resultados muestran un 99% de Sparsity en todas las películas, lo que da a entender que el vocabulario que se emplea es muy rico, lo cual es de esperar, debido que al tratarse de guiones de películas, quizá es menos frecuente que el vocabulario tienda a repetirse, como quizá podría ocurrir más en los libros.El alto grado de sparsity is a good sign that the topic modelling will be successful. Por lo que podemos aplicar LDA sin problemas.

```{r}
library(topicmodels)
# 4 topics are applied
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234)) 
chapters_lda
```

No obstante, para poder continuar con el análisis, es necesario poder trasformar el formato a tidy.

```{r}
chapter_topics <- tidy(chapters_lda)
chapter_topics
```

Al analizar los valores de beta en nuestro modelo de temas, es evidente que algunas palabras tienen asociaciones fuertes con temas particulares, lo que nos da pistas sobre su relevancia en el contexto de la historia. Sigamos adelante y visualicemos las cinco palabras principales de cada tema, lo que nos ofrecerá una visión clara y tangible de los motivos recurrentes a lo largo de la serie.

```{r}
library(forcats)
top_terms <- chapter_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic))  # Reordena los términos dentro de cada tema para la visualización

# Creando el gráfico
ggplot(top_terms, aes(x = beta, y = term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +  # Asegura que los términos estén correctamente ordenados dentro de las facetas
  labs(x = "Beta Value", y = "Top Terms") +
  theme_minimal() +
  theme(axis.title.y = element_blank(),  # Elimina el título del eje y para más limpieza
        strip.background = element_rect(fill = "lightblue"),  # Personaliza el fondo de las etiquetas de faceta
        strip.text = element_text(size = 12, color = "navy"))  # Personaliza el texto de las etiquetas de faceta
```

Echando un vistazo, parece que `harry` es un término estrella, encabezando los temas 3,4,5,6,7,8 y en el 2 siendo `Potter`. Esto no es sorprendente, ya que estamos hablando de Harry Potter. En el tema 1, palabras como `sir`,`dumbledore` y `professor`, acompañan a `harry`.

El tema 2 muestra una variedad diferente, con `ron` y `hermione` cerca de la cima, lo que sugiere que este tema podría tratar sobre los amigos y compañeros de Harry. Por otro lado, el tema 3 destaca `hagrid` y `hermione`junto a `harry`, quizás señalando momentos clave donde estos personajes interactúan o juegan roles significativos.

Finalmente, el tema 4, aparte de `harry`, destaca `dobby` y `time`. `Dobby` podría estar asociado a momentos emocionales y significativos de la saga, mientras que "time" podría estar relacionado con eventos críticos en la trama o incluso con el giratiempo de "El Prisionero de Azkaban".

### Each chapter to its book

Una vez, hemos hemos identificado esas palabras clave que definen con cada tema, el siguiente paso es intentar reconstruir el rompecabezas de los capítulos y asignar cada uno a su libro original. Para hacerlo, vamos a utilizar gamma, que nos dice la probabilidad de que un capítulo contenga ciertos temas. Piensa en gamma como la afinidad que tiene un capítulo por los temas que hemos identificado, algo así como el Sombrero Seleccionador decidiendo a qué casa de Hogwarts perteneces (si el que está leyendo esto no ha visto una película o ha leido los libros de Harry Potter, se ha perdido una referencia muy buena de explicar el `gamma`) . Con esta información, podemos predecir a qué libro podría pertenecer cada capítulo basándonos en sus temas dominantes.

```{r}
chapters_gamma <- tidy(chapters_lda, matrix = "gamma")
chapters_gamma
```

Al final, cada capítulo tendrá una serie de probabilidades asociadas a los temas, y, por ende, una sugerencia de a qué libro pertenece. Este paso es crucial porque nos permite ver cómo se agrupan los capítulos y validar nuestra comprensión temática de los libros. Si lo hacemos bien, ¡podremos ver cómo las piezas del rompecabezas se unen perfectamente!

```{r}
chapters_gamma <- chapters_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

chapters_gamma
```

And now, let's make a plot:

```{r,  fig.width=15, fig.asp=0.65, out.width='100%', preview = TRUE}
# reorder titles in order of topic 1, topic 2, etc before plotting
chapters_gamma %>%
  mutate(title = fct_reorder(title, gamma, .fun = sum)) %>%  # Reordena los títulos sumando las gammas
  ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +  # Usa el tema como color
  geom_boxplot() +
  facet_wrap(~ title, scales = "free_y") +  # Facetas por título con escalas libres en y
  labs(x = "Topic", y = expression(Gamma)) +
  theme_minimal() +  # Tema minimalista para limpiar el gráfico
  theme(legend.position = "none",  # Removemos la leyenda para evitar redundancia
        axis.title.x = element_blank(),  # Removemos el título del eje x para una apariencia más limpia
        axis.text.x = element_text(angle = 45, hjust = 1))
```

Podemos observar cómo algunos libros muestran una preferencia clara por ciertos temas, indicada por la altura de las cajas en cada tema. Por ejemplo, `Harry Potter and the Half-Blood Prince` tiene una caja muy alta en el tema representado en rojo, lo que podría indicar que este tema es especialmente prominente en ese libro. En contraste, `Harry Potter and the Chamber of Secrets` muestra una distribución más uniforme entre dos temas, con las cajas más altas en verde y azul, es decir, lo que representaria al topic 2 y 3

La dispersión de los puntos fuera de las cajas, es decir, los outliers, podría señalar capítulos que son excepcionalmente diferentes en su contenido temático en comparación con otros capítulos dentro de su mismo libro. Por ejemplo, hay algunos capítulos en "Harry Potter and the Order of the Phoenix" que parecen ser únicos en su perfil temático, dada la dispersión de los puntos.

Let's investigate further.

First, we should have a look at the topic that has been most associated with each chapter of the book.

```{r}
chapter_classifications <- chapters_gamma %>%
  group_by(title, chapter) %>%
  #we use slice max to order by gamma
  slice_max(gamma) %>%
  ungroup()

chapter_classifications
```

Parece que la mayoría de los capítulos de "La Cámara Secreta" tienen una vinculación casi total (valores de gamma muy cercanos a 1) con los temas 2 y 3. Por ejemplo, los capítulos `About the Chamber`, `Aragog`, y `Cornelius Fudge` están firmemente ligados al tema 3.

Algunos capítulos, como `Backfire` y `Car rescue`, están asociados casi completamente con el tema 2, lo que podría representar otra faceta central de la narrativa de este libro, tal vez más orientada hacia la acción y la aventura.

Sin embargo, hay capítulos como `Dobby's reward` y `Dobby's warning` que se desvían de esta tendencia, con una relación más fuerte con el tema 4, que puede estar reflejando aspectos más relacionados con el desarrollo del personaje o elementos de la trama que no están tan centrados en el misterio principal de la cámara.

Finalmente, para concluir nuestro análisis mágico, vamos a desvelar aquellos capítulos que, al parecer, han dado un giro inesperado en la trama y se han alineado con temas diferentes al hilo principal de su libro. Imagínalo como el Sombrero Seleccionador teniendo un día peculiar y asignando estudiantes a casas inesperadas (otra referencia muy buena que si eres fan seguro que has disfrutado y si no tienes ni idea de lo que hablo, pido perdon :))

First, we create a dataframe just with two columns: the book and its consensus topic.

```{r}
book_topics <- chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  slice_max(n, n = 1) %>% 
  ungroup() %>%
  transmute(consensus = title, topic)

book_topics
```

Second, we check with an inner join if any chapter is assigned to a topic different from the consensus.

```{r}
chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  filter(title != consensus)
```

La mayoría de los capítulos se asocian consistentemente con el tema 3, lo que sugiere que este tema captura elementos clave que son centrales en este libro particular. Es posible que el tema 3 encapsule el misterio y el suspense de la Cámara de los Secretos.

Sin embargo, también hay capítulos asociados con otros temas, como "Backfire" y "Car rescue" con el tema 2, y "Chamber of Secrets" con el tema 1, lo que nos dice que hay diversidad temática en el libro. Particularmente interesante es "Dobby's reward" y "Dobby's warning", que se desmarcan con una asociación al tema 4; esto puede indicar una subtrama o un enfoque en aspectos del desarrollo del personaje, especialmente en lo que respecta a Dobby.
